# Multilayer Perceptron with Stochastic Gradient Descent Optimization

### Project Overview
This project focuses on implementing a Multilayer Perceptron (MLP) and training it using a gradient optimization method combined with backpropagation. For this purpose, the Stochastic Gradient Descent (SGD) algorithm was chosen due to its efficiency in handling optimization tasks.

The goal of the project was to build a neural network capable of learning and accurately classifying handwritten digits. The model was trained and evaluated on The Digit Dataset, which consists of 1,797 grayscale images of digits ranging from 0 to 9. Each image is represented as a flattened vector of 64 pixels, with pixel values ranging from 0 to 16.

